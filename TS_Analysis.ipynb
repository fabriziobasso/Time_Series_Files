{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabriziobasso/Time_Series_Files/blob/main/TS_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0l8OsIhMVeZ"
      },
      "source": [
        "# **Time series forecasting**\n",
        "This tutorial is an introduction to time series forecasting using TensorFlow. It builds a few different styles of models including Convolutional and Recurrent Neural Networks (CNNs and RNNs).\n",
        "\n",
        "This is covered in two main parts, with subsections:\n",
        "\n",
        "* Forecast for a single time step:\n",
        "  * A single feature.\n",
        "  * All features.\n",
        "* Forecast multiple steps:\n",
        "  * Single-shot: Make the predictions all at once.\n",
        "  * Autoregressive: Make one prediction at a time and feed the output back to the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7wvddu4riye"
      },
      "source": [
        "## 1.0 Install Packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NssaJaAQngVv"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install tensorflow-addons\n",
        "#!pip install shap\n",
        "#!pip install eli5\n",
        "#!pip install tf-nightly\n",
        "#!pip install -U scikit-learn==1.2.0\n",
        "!pip install catboost\n",
        "#!pip install haversine\n",
        "#!pip install umap-learn\n",
        "#!pip install reverse_geocoder\n",
        "#!pip install --upgrade protobuf\n",
        "!pip install colorama\n",
        "!pip install imbalanced-learn\n",
        "!pip install optuna\n",
        "!pip install optuna-integration\n",
        "!pip install pygam\n",
        "!pip install keras-tuner --upgrade\n",
        "!pip install pycaret"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aEn1w28Dxbh"
      },
      "source": [
        "## **2.0 Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "remwXrYBDxbh",
        "outputId": "55ab92ce-9867-4c6f-abd6-0adac3185f45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importing started...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0x10 but this version of numpy is 0xf"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-dc28cef3901d>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjoblib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomOverSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfunctools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_output\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_SetOutputMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m from .utils._tags import (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdeprecation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdiscovery\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mall_estimators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreadpool_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m from .validation import (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mthreadpoolctl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    483\u001b[0m from ._warnings_errors import (ConstantInputWarning, NearConstantInputWarning,\n\u001b[1;32m    484\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[0;32m--> 485\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_stats_py\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_variation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvariation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNumpyVersion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msuppress_warnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/testing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0munittest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTestCase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_private\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_private\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_assert_valid_refcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_gen_alignment_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_private\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mextbuild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorators\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/testing/_private/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m from numpy.core import(\n\u001b[1;32m     22\u001b[0m      intp, float32, empty, arange, array_repr, ndarray, isnat, array)\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlapack_lite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "#importing modules\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import time\n",
        "t = time.time()\n",
        "\n",
        "print('Importing started...')\n",
        "\n",
        "# basic moduele\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "#from scipy import stats\n",
        "from random import randint\n",
        "import random\n",
        "import math\n",
        "import os\n",
        "import gc\n",
        "import pickle\n",
        "from glob import glob\n",
        "from IPython import display as ipd\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from joblib import dump, load\n",
        "import sklearn as sk\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
        "from functools import partial\n",
        "import itertools\n",
        "from itertools import combinations\n",
        "import IPython\n",
        "import IPython.display\n",
        "\n",
        "# visualization moduels\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib.patches as mpatches\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import missingno as msno\n",
        "import imblearn\n",
        "\n",
        "# Palette Setup\n",
        "colors = ['#FB5B68','#FFEB48','#2676A1','#FFBDB0',]\n",
        "colormap_0 = mpl.colors.LinearSegmentedColormap.from_list(\"\",colors)\n",
        "palette_1 = sns.color_palette(\"coolwarm\", as_cmap=True)\n",
        "palette_2 = sns.color_palette(\"YlOrBr\", as_cmap=True)\n",
        "palette_3 = sns.light_palette(\"red\", as_cmap=True)\n",
        "palette_4 = sns.color_palette(\"viridis\", as_cmap=True)\n",
        "palette_5 = sns.color_palette(\"rocket\", as_cmap=True)\n",
        "palette_6 = sns.color_palette(\"GnBu\", as_cmap=True)\n",
        "palette_7 = sns.color_palette(\"tab20c\", as_cmap=False)\n",
        "palette_8 = sns.color_palette(\"Set2\", as_cmap=False)\n",
        "\n",
        "palette_custom = ['#fbb4ae','#b3cde3','#ccebc5','#decbe4','#fed9a6','#ffffcc','#e5d8bd','#fddaec','#f2f2f2']\n",
        "palette_9 = sns.color_palette(palette_custom, as_cmap=False)\n",
        "\n",
        "mpl.rcParams['axes.spines.left'] = True\n",
        "mpl.rcParams['axes.spines.right'] = False\n",
        "mpl.rcParams['axes.spines.top'] = False\n",
        "mpl.rcParams['axes.spines.bottom'] = True\n",
        "\n",
        "# Style Import\n",
        "from colorama import Style, Fore\n",
        "red = Style.BRIGHT + Fore.RED\n",
        "blu = Style.BRIGHT + Fore.BLUE\n",
        "mgt = Style.BRIGHT + Fore.MAGENTA\n",
        "gld = Style.BRIGHT + Fore.YELLOW\n",
        "res = Style.RESET_ALL\n",
        "\n",
        "# preprocessing modules\n",
        "from sklearn.model_selection import (train_test_split,\n",
        "                                     KFold,\n",
        "                                     StratifiedKFold,\n",
        "                                     cross_val_score,\n",
        "                                     GroupKFold,\n",
        "                                     GridSearchCV,\n",
        "                                     RepeatedStratifiedKFold)\n",
        "\n",
        "from sklearn.preprocessing import (LabelEncoder,\n",
        "                                   StandardScaler,\n",
        "                                   MinMaxScaler,\n",
        "                                   OrdinalEncoder,\n",
        "                                   RobustScaler,\n",
        "                                   PowerTransformer,\n",
        "                                   OneHotEncoder,\n",
        "                                   LabelEncoder,\n",
        "                                   OrdinalEncoder,\n",
        "                                   PolynomialFeatures)\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "\n",
        "# metrics\n",
        "from sklearn.metrics import (mean_squared_error,\n",
        "                             r2_score,\n",
        "                             mean_absolute_error,\n",
        "                             mean_absolute_percentage_error,\n",
        "                             classification_report,\n",
        "                             confusion_matrix,\n",
        "                             ConfusionMatrixDisplay,\n",
        "                             multilabel_confusion_matrix,\n",
        "                             accuracy_score,\n",
        "                             roc_auc_score,\n",
        "                             auc,\n",
        "                             roc_curve,\n",
        "                             log_loss)\n",
        "\n",
        "\n",
        "# modeling algos\n",
        "from sklearn.linear_model import (LogisticRegression,\n",
        "                                  Lasso,\n",
        "                                  ridge_regression,\n",
        "                                  LinearRegression,\n",
        "                                  Ridge,\n",
        "                                  RidgeCV,\n",
        "                                  ElasticNet,\n",
        "                                  BayesianRidge,\n",
        "                                  TweedieRegressor,\n",
        "                                  ARDRegression,\n",
        "                                  PoissonRegressor,\n",
        "                                  GammaRegressor)\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "\n",
        "from sklearn.ensemble import (AdaBoostRegressor,\n",
        "                              RandomForestRegressor,\n",
        "                              RandomForestClassifier,\n",
        "                              VotingRegressor,\n",
        "                              GradientBoostingRegressor,\n",
        "                              StackingRegressor,\n",
        "                              HistGradientBoostingClassifier,\n",
        "                              ExtraTreesClassifier)\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# Other Models\n",
        "from pygam import LogisticGAM, s, te\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBRegressor, XGBClassifier\n",
        "import lightgbm as lgb\n",
        "from lightgbm import (LGBMRegressor,\n",
        "                      LGBMClassifier,\n",
        "                      early_stopping,\n",
        "                      record_evaluation,\n",
        "                      log_evaluation)\n",
        "\n",
        "import catboost as cat\n",
        "from catboost import CatBoost, CatBoostRegressor\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "from catboost.utils import get_roc_curve\n",
        "\n",
        "from lightgbm import early_stopping\n",
        "# check installed version\n",
        "import pycaret\n",
        "\n",
        "from sklearn.base import clone ## sklearn base models for stacked ensemble model\n",
        "from sklearn.calibration import CalibratedClassifierCV, CalibrationDisplay\n",
        "\n",
        "#Interpretiability of the model\n",
        "#import shap\n",
        "#import eli5\n",
        "#from eli5.sklearn import PermutationImportance\n",
        "\n",
        "\n",
        "## miss\n",
        "from sklearn.pipeline import (make_pipeline,\n",
        "                              Pipeline)\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow_addons as tfa\n",
        "from keras.utils import FeatureSpace\n",
        "\n",
        "# Import libraries for Hypertuning\n",
        "import kerastuner as kt\n",
        "from kerastuner.tuners import RandomSearch, GridSearch, BayesianOptimization\n",
        "# Model Tuning tools:\n",
        "import optuna\n",
        "from optuna.integration import TFKerasPruningCallback\n",
        "from optuna.trial import TrialState\n",
        "from optuna.visualization import plot_intermediate_values\n",
        "from optuna.visualization import plot_optimization_history\n",
        "from optuna.visualization import plot_param_importances\n",
        "from optuna.visualization import plot_contour\n",
        "\n",
        "SEED = 1984\n",
        "N_SPLITS = 10\n",
        "\n",
        "print('Done, All the required modules are imported. Time elapsed: {} sec'.format(time.time()-t))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAB1kcdZDxbi"
      },
      "outputs": [],
      "source": [
        "# Check Versions:\n",
        "print(\"CHECK VERSIONS:\")\n",
        "print(f\"sns: {sns.__version__}\")\n",
        "print(f\"mpl: {mpl.__version__}\")\n",
        "print(f\"tensorflow: {tf.__version__}\")\n",
        "print(f\"pandas: {pd.__version__}\")\n",
        "print(f\"numpy: {np.__version__}\")\n",
        "print(f\"scikit-learn: {sk.__version__}\")\n",
        "#print(f\"statsmodels: {stm.__version__}\")\n",
        "print(f\"missingno: {msno.__version__}\")\n",
        "#print(f\"TF-addon: {tfa.__version__}\")\n",
        "print(f\"Inbalance_Learning: {imblearn.__version__}\")\n",
        "print(f\"XGBoost: {xgb.__version__}\")\n",
        "print(f\"CatBoost: {cat.__version__}\")\n",
        "print(f\"PyCaret: {pycaret.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFAGryGDDxbi"
      },
      "source": [
        "Seeding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Yv_lYmRDxbi"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(0)\n",
        "np.random.seed(0)\n",
        "random.seed(42)\n",
        "\n",
        "SEED = 42\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    tf.compat.v1.set_random_seed(seed)\n",
        "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
        "    tf.compat.v1.keras.backend.set_session(sess)\n",
        "\n",
        "seed_everything(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STsF4sfaDxbj"
      },
      "source": [
        "### **2.1 Connect Drives**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D_HgNN1Dxbj"
      },
      "source": [
        "Verify System:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nucekKRDxbj"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xwI9dKRDxbj"
      },
      "source": [
        "Connect to Google Drive:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnZHOmpDDxbj"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Connect to Colab:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcufbAJ7JLTp"
      },
      "outputs": [],
      "source": [
        "folder_data = \"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/TS_Models_tf\"\n",
        "folder_train_valid = \"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/TS_Models_tf/train_valid_data\"\n",
        "models_folders = \"/content/drive/MyDrive/Exercises/Time_Series_Studies/Models/TS_Models_tf\"\n",
        "folders_nn = \"/content/drive/MyDrive/Exercises/Time_Series_Studies/Models/TS_Models_tf/neural_networks/\"\n",
        "\n",
        "list_directories = [folder_data,folder_train_valid,models_folders,folders_nn]\n",
        "\n",
        "for path in list_directories:\n",
        "  try:\n",
        "      os.mkdir(path)\n",
        "  except OSError as error:\n",
        "      print(f\"{path} already exists\")\n",
        "\n",
        "\n",
        "os.chdir(folder_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3FQ7HXrVOKh"
      },
      "source": [
        "## **3.0 Import Data**\n",
        "\n",
        "## The weather dataset\n",
        "\n",
        "This tutorial uses a <a href=\"https://www.bgc-jena.mpg.de/wetter/\" class=\"external\">weather time series dataset</a> recorded by the <a href=\"https://www.bgc-jena.mpg.de\" class=\"external\">Max Planck Institute for Biogeochemistry</a>.\n",
        "\n",
        "This dataset contains 14 different features such as air temperature, atmospheric pressure, and humidity. These were collected every 10 minutes, beginning in 2003. For efficiency, you will use only the data collected between 2009 and 2016. This section of the dataset was prepared by François Chollet for his book <a href=\"https://www.manning.com/books/deep-learning-with-python\" class=\"external\">Deep Learning with Python</a>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wm9CVkh2VKAv"
      },
      "outputs": [],
      "source": [
        "zip_path = tf.keras.utils.get_file(\n",
        "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
        "    fname='jena_climate_2009_2016.csv.zip',\n",
        "    extract=True)\n",
        "csv_path, _ = os.path.splitext(zip_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quwHynpTV2S2"
      },
      "source": [
        "This tutorial will just deal with **hourly predictions**, so start by sub-sampling the data from 10-minute intervals to one-hour intervals:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Msdm3NjbVj5G"
      },
      "outputs": [],
      "source": [
        "csv_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-eFeo5nVmYo"
      },
      "outputs": [],
      "source": [
        "custom_date_parser = lambda x: datetime.strptime(x, '%d.%m.%Y %H:%M:%S')\n",
        "\n",
        "df = pd.read_csv(csv_path,\n",
        "                 parse_dates=['Date Time'],\n",
        "                 date_parser=custom_date_parser)\n",
        "# Slice [start:stop:step], starting from index 5 take every 6th record. --> This is because\n",
        "# only Hourly data are considered while the original dataset is in minutes\n",
        "\n",
        "df = df[5::6]\n",
        "date_time = df.pop('Date Time')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzIcxZ68XWXs"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NCQpDndXmJ0"
      },
      "outputs": [],
      "source": [
        "date_time[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sJjBRGGrrRu"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRzj1inMfgcO"
      },
      "source": [
        "Here is the evolution of a few features over time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_uc0nmuYj6u"
      },
      "outputs": [],
      "source": [
        "print(list(df.columns))\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vg5XIc5tfNlG"
      },
      "outputs": [],
      "source": [
        "plot_cols = ['wv (m/s)', 'max. wv (m/s)', 'rho (g/m**3)']\n",
        "plot_features = df[plot_cols]\n",
        "plot_features.index = date_time\n",
        "_ = plot_features.plot(subplots=True)\n",
        "\n",
        "plot_features = df[plot_cols][:480]\n",
        "plot_features.index = date_time[:480]\n",
        "_ = plot_features.plot(subplots=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXWLG0_WBhZS"
      },
      "source": [
        "### 3.1 Inspect and cleanup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhmZXJew6GlS"
      },
      "source": [
        "Next, look at the statistics of the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h510pgKVrrai"
      },
      "outputs": [],
      "source": [
        "df.describe().transpose()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tvr229-4YStj"
      },
      "outputs": [],
      "source": [
        "df.isna().any().any()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzOTnWOoWMGK"
      },
      "source": [
        "### 3.2 Wind velocity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i47LiW5DCVsP"
      },
      "source": [
        "One thing that should stand out is the `min` value of the wind velocity (`wv (m/s)`) and the maximum value (`max. wv (m/s)`) columns. This `-9999` is likely erroneous.\n",
        "\n",
        "There's a separate wind direction column, so the velocity should be greater than zero (`>=0`). Replace it with zeros:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8TGqaCBZb_i"
      },
      "outputs": [],
      "source": [
        "wv = df['wv (m/s)']\n",
        "bad_wv = wv == -9999.0\n",
        "wv[bad_wv] = df['wv (m/s)'].median()\n",
        "\n",
        "max_wv = df['max. wv (m/s)']\n",
        "bad_max_wv = max_wv == -9999.0\n",
        "max_wv[bad_max_wv] = df['max. wv (m/s)'].median()\n",
        "\n",
        "# The above inplace edits are reflected in the DataFrame.\n",
        "df[['wv (m/s)','max. wv (m/s)']].min()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtmu2IBPgPG8"
      },
      "source": [
        "## 4.0 Feature engineering\n",
        "\n",
        "Before diving in to build a model, it's important to understand your data and be sure that you're passing the model appropriately formatted data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYyEaqiD6j4s"
      },
      "source": [
        "#### Wind\n",
        "The last column of the data, `wd (deg)`—gives the wind direction in units of degrees. Angles do not make good model inputs: 360° and 0° should be close to each other and wrap around smoothly. Direction shouldn't matter if the wind is not blowing.\n",
        "\n",
        "Right now the distribution of wind data looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOgpg2OzaTra"
      },
      "outputs": [],
      "source": [
        "plt.hist2d(df['wd (deg)'], df['wv (m/s)'], bins=(80, 80), vmax=200)\n",
        "plt.colorbar()\n",
        "plt.xlabel('Wind Direction [deg]')\n",
        "plt.ylabel('Wind Velocity [m/s]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSYnyDe4sW9U"
      },
      "source": [
        "But this will be easier for the model to interpret if you convert the wind direction and velocity columns to a wind vector:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXfaMgigaiy_"
      },
      "outputs": [],
      "source": [
        "wv = df.pop('wv (m/s)')\n",
        "max_wv = df.pop('max. wv (m/s)')\n",
        "\n",
        "# Convert to radians.\n",
        "wd_rad = df.pop('wd (deg)')*np.pi / 180\n",
        "\n",
        "# Calculate the wind x and y components.\n",
        "df['Wx'] = wv*np.cos(wd_rad)\n",
        "df['Wy'] = wv*np.sin(wd_rad)\n",
        "\n",
        "# Calculate the max wind x and y components.\n",
        "df['max Wx'] = max_wv*np.cos(wd_rad)\n",
        "df['max Wy'] = max_wv*np.sin(wd_rad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iI0zDoxWDyB"
      },
      "source": [
        "The distribution of wind vectors is much simpler for the model to correctly interpret:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMgCG5o2SYKD"
      },
      "outputs": [],
      "source": [
        "plt.hist2d(df['Wx'], df['Wy'], bins=(80, 80), vmax=400)\n",
        "plt.colorbar()\n",
        "plt.xlabel('Wind X [m/s]')\n",
        "plt.ylabel('Wind Y [m/s]')\n",
        "ax = plt.gca()\n",
        "ax.axis('tight');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8im1ttOWlRB"
      },
      "source": [
        "#### Time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YE21HKK40zQ"
      },
      "source": [
        "Similarly, the `Date Time` column is very useful, but not in this string form. Start by converting it to seconds:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIFf-VjMfnh3"
      },
      "outputs": [],
      "source": [
        "timestamp_s = date_time.map(pd.Timestamp.timestamp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wO7_jc3zKP5l"
      },
      "source": [
        "Similar to the wind direction, the time in seconds is not a useful model input. Being weather data, it has clear daily and yearly periodicity. There are many ways you could deal with periodicity.\n",
        "\n",
        "You can get usable signals by using sine and cosine transforms to clear \"Time of day\" and \"Time of year\" signals:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ljTa1alsgYQ"
      },
      "outputs": [],
      "source": [
        "day = 24*60*60\n",
        "year = (365.2425)*day\n",
        "month = (30.437)*day\n",
        "\n",
        "df['Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
        "df['Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
        "df['Month sin'] = np.sin(timestamp_s * (2 * np.pi / month))\n",
        "df['Month cos'] = np.cos(timestamp_s * (2 * np.pi / month))\n",
        "df['Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n",
        "df['Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJKkNJ1FsgVi"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,3))\n",
        "plt.plot(np.array(df['Day sin'])[:730])\n",
        "plt.plot(np.array(df['Day cos'])[:730])\n",
        "plt.plot(np.array(df['Month sin'])[:730])\n",
        "plt.plot(np.array(df['Month cos'])[:730])\n",
        "plt.plot(np.array(df['Year sin'])[:730])\n",
        "plt.plot(np.array(df['Year cos'])[:730])\n",
        "plt.xlabel('Time [h]')\n",
        "plt.grid(linestyle='--',alpha=0.6)\n",
        "plt.title('Time of day signal');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkg7Vnt9K_Ql"
      },
      "source": [
        "This gives the model access to the most important frequency features. In this case you knew ahead of time which frequencies were important.\n",
        "\n",
        "If you don't have that information, you can determine which frequencies are important by extracting features with <a href=\"https://en.wikipedia.org/wiki/Fast_Fourier_transform\" class=\"external\">Fast Fourier Transform</a>. To check the assumptions, here is the `tf.signal.rfft` of the temperature over time. Note the obvious peaks at frequencies near `1/year` and `1/day`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjbDKvD3sgSx"
      },
      "outputs": [],
      "source": [
        "fft = tf.signal.rfft(df['T (degC)'])\n",
        "f_per_dataset = np.arange(0, len(fft))\n",
        "\n",
        "n_samples_h = len(df['T (degC)'])\n",
        "hours_per_year = 24*365.2524\n",
        "years_per_dataset = n_samples_h/(hours_per_year)\n",
        "\n",
        "f_per_year = f_per_dataset/years_per_dataset\n",
        "plt.step(f_per_year, np.abs(fft))\n",
        "plt.xscale('log')\n",
        "plt.ylim(0, 400000)\n",
        "plt.xlim([0.1, max(plt.xlim())])\n",
        "plt.xticks([1, 365.2524], labels=['1/Year', '1/day'])\n",
        "_ = plt.xlabel('Frequency (log scale)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rbL8bSGDHy3"
      },
      "source": [
        "### Split the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoFJZmXBaxCc"
      },
      "source": [
        "You'll use a `(70%, 20%, 10%)` split for the training, validation, and test sets. Note the data is **not** being randomly shuffled before splitting. This is for two reasons:\n",
        "\n",
        "1. It ensures that chopping the data into windows of consecutive samples is still possible.\n",
        "2. It ensures that the validation/test results are more realistic, being evaluated on the data collected after the model was trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ia-MPAHxbInX"
      },
      "outputs": [],
      "source": [
        "column_indices = {name: i for i, name in enumerate(df.columns)}\n",
        "\n",
        "n = len(df)\n",
        "train_df = df[0:int(n*0.7)]\n",
        "val_df = df[int(n*0.7):int(n*0.9)]\n",
        "test_df = df[int(n*0.9):]\n",
        "\n",
        "num_features = df.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGvmCAMBsgP7"
      },
      "outputs": [],
      "source": [
        "print(f\"Train shape: {train_df.shape}\\nValid shape: {val_df.shape}\\nTest shape: {test_df.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eFckdUUHWmT"
      },
      "source": [
        "### Normalize the data\n",
        "\n",
        "It is important to scale features before training a neural network. Normalization is a common way of doing this scaling: subtract the mean and divide by the standard deviation of each feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxbIic5TMlxx"
      },
      "source": [
        "The mean and standard deviation should only be computed using the training data so that the models have no access to the values in the validation and test sets.\n",
        "\n",
        "It's also arguable that the model shouldn't have access to future values in the training set when training, and that this normalization should be done using moving averages. That's not the focus of this tutorial, and the validation and test sets ensure that you get (somewhat) honest metrics. So, in the interest of simplicity this tutorial uses a simple average."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eji6njXvHusN"
      },
      "outputs": [],
      "source": [
        "train_mean = train_df.mean()\n",
        "train_std = train_df.std()\n",
        "\n",
        "train_df = (train_df - train_mean) / train_std\n",
        "val_df = (val_df - train_mean) / train_std\n",
        "test_df = (test_df - train_mean) / train_std"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6ufs8kk9JQw"
      },
      "source": [
        "Now, peek at the distribution of the features. Some features do have long tails, but there are no obvious errors like the `-9999` wind velocity value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0UYEnkwm8Fe"
      },
      "outputs": [],
      "source": [
        "df_std = (df - train_mean) / train_std\n",
        "df_std = df_std.melt(var_name='Column', value_name='Normalized')\n",
        "plt.figure(figsize=(12, 6))\n",
        "ax = sns.violinplot(x='Column', y='Normalized', data=df_std)\n",
        "_ = ax.set_xticklabels(df.keys(), rotation=90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBBmdxZ2HgfJ"
      },
      "source": [
        "## Data windowing\n",
        "\n",
        "The models in this tutorial will make a set of predictions based on a window of consecutive samples from the data.\n",
        "\n",
        "The main features of the input windows are:\n",
        "\n",
        "- The width (number of time steps) of the input and label windows.\n",
        "- The time offset between them.\n",
        "- Which features are used as inputs, labels, or both.\n",
        "\n",
        "This tutorial builds a variety of models (including Linear, DNN, CNN and RNN models), and uses them for both:\n",
        "\n",
        "- *Single-output*, and *multi-output* predictions.\n",
        "- *Single-time-step* and *multi-time-step* predictions.\n",
        "\n",
        "This section focuses on implementing the data windowing so that it can be reused for all of those models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAhGUVx1jtOy"
      },
      "source": [
        "Depending on the task and type of model you may want to generate a variety of data windows. Here are some examples:\n",
        "\n",
        "1. For example, to make a single prediction 24 hours into the future, given 24 hours of history, you might define a window like this:\n",
        "\n",
        "  ![One prediction 24 hours into the future.](https://drive.google.com/file/d/16Z0Bvjr6-ugxZaFjVZlh759IlDT1-Nuy/view?usp=sharing)\n",
        "\n",
        "  <img src=\"/images/1_step.png\">\n",
        "\n",
        "2. A model that makes a prediction one hour into the future, given six hours of history, would need a window like this:\n",
        "\n",
        "  ![One prediction one hour into the future.](https://github.com/fabriziobasso/Closet_Index_Tracking/blob/master/images/raw_window_1h.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa2BbfNZt8wy"
      },
      "source": [
        "The rest of this section defines a `WindowGenerator` class. This class can:\n",
        "\n",
        "1. Handle the indexes and offsets as shown in the diagrams above.\n",
        "1. Split windows of features into `(features, labels)` pairs.\n",
        "2. Plot the content of the resulting windows.\n",
        "3. Efficiently generate batches of these windows from the training, evaluation, and test data, using `tf.data.Dataset`s."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfx3jGjyziUF"
      },
      "source": [
        "### 1. Indexes and offsets\n",
        "\n",
        "Start by creating the `WindowGenerator` class. The `__init__` method includes all the necessary logic for the input and label indices.\n",
        "\n",
        "It also takes the training, evaluation, and test DataFrames as input. These will be converted to `tf.data.Dataset`s of windows later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kem30j8QHxyW"
      },
      "outputs": [],
      "source": [
        "class WindowGenerator():\n",
        "  def __init__(self, input_width, label_width, shift,\n",
        "               train_df=train_df, val_df=val_df, test_df=test_df,\n",
        "               label_columns=None):\n",
        "    # Store the raw data.\n",
        "    self.train_df = train_df\n",
        "    self.val_df = val_df\n",
        "    self.test_df = test_df\n",
        "\n",
        "    # Work out the label column indices.\n",
        "    self.label_columns = label_columns\n",
        "    if label_columns is not None:\n",
        "      self.label_columns_indices = {name: i for i, name in\n",
        "                                    enumerate(label_columns)}\n",
        "    self.column_indices = {name: i for i, name in\n",
        "                           enumerate(train_df.columns)}\n",
        "\n",
        "    # Work out the window parameters.\n",
        "    self.input_width = input_width\n",
        "    self.label_width = label_width\n",
        "    self.shift = shift\n",
        "\n",
        "    self.total_window_size = input_width + shift\n",
        "\n",
        "    self.input_slice = slice(0, input_width)\n",
        "    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
        "\n",
        "    self.label_start = self.total_window_size - self.label_width\n",
        "    self.labels_slice = slice(self.label_start, None)\n",
        "    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
        "\n",
        "  def __repr__(self):\n",
        "    return '\\n'.join([\n",
        "        f'Total window size: {self.total_window_size}',\n",
        "        f'Input indices: {self.input_indices}',\n",
        "        f'Label indices: {self.label_indices}',\n",
        "        f'Label column name(s): {self.label_columns}'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PijpDoi9U1LS"
      },
      "source": [
        "Here is code to create the 2 windows shown in the diagrams at the start of this section:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72UhPSt1sgNN"
      },
      "outputs": [],
      "source": [
        "w1 = WindowGenerator(input_width=24, label_width=1, shift=24,\n",
        "                     label_columns=['T (degC)'])\n",
        "w1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-9aLJLtU87q"
      },
      "outputs": [],
      "source": [
        "w2 = WindowGenerator(input_width=6, label_width=1, shift=1,\n",
        "                     label_columns=['T (degC)'])\n",
        "w2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJaUyTWQJd-L"
      },
      "source": [
        "### 2. Split\n",
        "\n",
        "Given a list of consecutive inputs, the `split_window` method will convert them to a window of inputs and a window of labels.\n",
        "\n",
        "The example `w2` you define earlier will be split like this:\n",
        "\n",
        "![The initial window is all consecutive samples, this splits it into an (inputs, labels) pairs](https://github.com/fabriziobasso/Closet_Index_Tracking/blob/master/images/split_window.png?raw=1)\n",
        "\n",
        "This diagram doesn't show the `features` axis of the data, but this `split_window` function also handles the `label_columns` so it can be used for both the single output and multi-output examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4KbxfzqkXPW"
      },
      "outputs": [],
      "source": [
        "def split_window(self, features):\n",
        "  inputs = features[:, self.input_slice, :]\n",
        "  labels = features[:, self.labels_slice, :]\n",
        "  if self.label_columns is not None:\n",
        "    labels = tf.stack(\n",
        "        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
        "        axis=-1)\n",
        "\n",
        "  # Slicing doesn't preserve static shape information, so set the shapes\n",
        "  # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
        "  inputs.set_shape([None, self.input_width, None])\n",
        "  labels.set_shape([None, self.label_width, None])\n",
        "\n",
        "  return inputs, labels\n",
        "\n",
        "WindowGenerator.split_window = split_window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8AB3i4HU-Va"
      },
      "outputs": [],
      "source": [
        "# Stack three slices, the length of the total window.\n",
        "example_window = tf.stack([np.array(train_df[:w2.total_window_size]),\n",
        "                           np.array(train_df[100:100+w2.total_window_size]),\n",
        "                           np.array(train_df[200:200+w2.total_window_size])])\n",
        "\n",
        "example_inputs, example_labels = w2.split_window(example_window)\n",
        "\n",
        "print('All shapes are: (batch, time, features)')\n",
        "print(f'Window shape: {example_window.shape}')\n",
        "print(f'Inputs shape: {example_inputs.shape}')\n",
        "print(f'Labels shape: {example_labels.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDzAqtqp4Cgq"
      },
      "outputs": [],
      "source": [
        "train_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFZukGXrJoGo"
      },
      "source": [
        "### 3. Plot\n",
        "\n",
        "Here is a plot method that allows a simple visualization of the split window:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmgd1qkYUWT7"
      },
      "outputs": [],
      "source": [
        "w2.example = example_inputs, example_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIrYccI-Hm3B"
      },
      "outputs": [],
      "source": [
        "def plot(self, model=None, plot_col='T (degC)', max_subplots=3):\n",
        "  inputs, labels = self.example\n",
        "  plt.figure(figsize=(12, 8))\n",
        "  plot_col_index = self.column_indices[plot_col]\n",
        "  max_n = min(max_subplots, len(inputs))\n",
        "  for n in range(max_n):\n",
        "    plt.subplot(max_n, 1, n+1)\n",
        "    plt.ylabel(f'{plot_col} [normed]')\n",
        "    plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
        "             label='Inputs', marker='.', zorder=-10)\n",
        "\n",
        "    if self.label_columns:\n",
        "      label_col_index = self.label_columns_indices.get(plot_col, None)\n",
        "    else:\n",
        "      label_col_index = plot_col_index\n",
        "\n",
        "    if label_col_index is None:\n",
        "      continue\n",
        "\n",
        "    plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
        "                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
        "    if model is not None:\n",
        "      predictions = model(inputs)\n",
        "      plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
        "                  marker='X', edgecolors='k', label='Predictions',\n",
        "                  c='#ff7f0e', s=64)\n",
        "\n",
        "    if n == 0:\n",
        "      plt.legend()\n",
        "\n",
        "  plt.xlabel('Time [h]')\n",
        "\n",
        "WindowGenerator.plot = plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXvctEuK68vX"
      },
      "source": [
        "This plot aligns inputs, labels, and (later) predictions based on the time that the item refers to:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjTqUnglOOni"
      },
      "outputs": [],
      "source": [
        "w2.plot(max_subplots=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqiqcPOldPG6"
      },
      "source": [
        "You can plot the other columns, but the example window `w2` configuration only has labels for the `T (degC)` column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBRe4wnlfCH8"
      },
      "outputs": [],
      "source": [
        "w2.plot(plot_col='p (mbar)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCvD-UaUzYMw"
      },
      "source": [
        "### 4. Create `tf.data.Dataset`s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLO3SFR9Osdf"
      },
      "source": [
        "Finally, this `make_dataset` method will take a time series DataFrame and convert it to a `tf.data.Dataset` of `(input_window, label_window)` pairs using the `tf.keras.utils.timeseries_dataset_from_array` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35qoSQeRVfJg"
      },
      "outputs": [],
      "source": [
        "def make_dataset(self, data):\n",
        "  data = np.array(data, dtype=np.float32)\n",
        "  ds = tf.keras.utils.timeseries_dataset_from_array(\n",
        "      data=data,\n",
        "      targets=None,\n",
        "      sequence_length=self.total_window_size,\n",
        "      sequence_stride=1,\n",
        "      shuffle=True,\n",
        "      batch_size=32,)\n",
        "\n",
        "  ds = ds.map(self.split_window)\n",
        "\n",
        "  return ds\n",
        "\n",
        "WindowGenerator.make_dataset = make_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvsxQwJaCift"
      },
      "source": [
        "The `WindowGenerator` object holds training, validation, and test data.\n",
        "\n",
        "Add properties for accessing them as `tf.data.Dataset`s using the `make_dataset` method you defined earlier. Also, add a standard example batch for easy access and plotting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jZ2KkqGCfzu"
      },
      "outputs": [],
      "source": [
        "@property\n",
        "def train(self):\n",
        "  return self.make_dataset(self.train_df)\n",
        "\n",
        "@property\n",
        "def val(self):\n",
        "  return self.make_dataset(self.val_df)\n",
        "\n",
        "@property\n",
        "def test(self):\n",
        "  return self.make_dataset(self.test_df)\n",
        "\n",
        "@property\n",
        "def example(self):\n",
        "  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
        "  result = getattr(self, '_example', None)\n",
        "  if result is None:\n",
        "    # No example batch was found, so get one from the `.train` dataset\n",
        "    result = next(iter(self.train))\n",
        "    # And cache it for next time\n",
        "    self._example = result\n",
        "  return result\n",
        "\n",
        "WindowGenerator.train = train\n",
        "WindowGenerator.val = val\n",
        "WindowGenerator.test = test\n",
        "WindowGenerator.example = example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF_Vj6Iw3Y2w"
      },
      "source": [
        "Now, the `WindowGenerator` object gives you access to the `tf.data.Dataset` objects, so you can easily iterate over the data.\n",
        "\n",
        "The `Dataset.element_spec` property tells you the structure, data types, and shapes of the dataset elements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daJ0-U383YVs"
      },
      "outputs": [],
      "source": [
        "# Each element is an (inputs, label) pair.\n",
        "w2.train.element_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKTx3_Z7ua-n"
      },
      "source": [
        "Iterating over a `Dataset` yields concrete batches:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gtKXEgf4Iml"
      },
      "outputs": [],
      "source": [
        "for example_inputs, example_labels in w2.train.take(1):\n",
        "  print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n",
        "  print(f'Labels shape (batch, time, features): {example_labels.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyuGuJUgjUK3"
      },
      "source": [
        "## Single step models\n",
        "\n",
        "The simplest model you can build on this sort of data is one that predicts a single feature's value—1 time step (one hour) into the future based only on the current conditions.\n",
        "\n",
        "So, start by building models to predict the `T (degC)` value one hour into the future.\n",
        "\n",
        "[Predict the next time step](https://drive.google.com/file/d/19qjyi2tBmpnRC2tuEReH9YpK_mEnBdZN/view?usp=sharing)\n",
        "\n",
        "Configure a `WindowGenerator` object to produce these single-step `(input, label)` pairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5QX1G1JTPCr"
      },
      "outputs": [],
      "source": [
        "single_step_window = WindowGenerator(\n",
        "    input_width=1, label_width=1, shift=1,\n",
        "    label_columns=['T (degC)'])\n",
        "single_step_window"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKTm8ajVGw4N"
      },
      "source": [
        "The `window` object creates `tf.data.Dataset`s from the training, validation, and test sets, allowing you to easily iterate over batches of data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Do4ILUaBF8oc"
      },
      "outputs": [],
      "source": [
        "for example_inputs, example_labels in single_step_window.train.take(1):\n",
        "  print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n",
        "  print(f'Labels shape (batch, time, features): {example_labels.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u24DAtdCd4Qb"
      },
      "source": [
        "**TS Data loader from Hands-on Texbook**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHfAEGdMFdMz"
      },
      "outputs": [],
      "source": [
        "train_ds = tf.keras.utils.timeseries_dataset_from_array(\n",
        "                                                        train_df.to_numpy(),\n",
        "                                                        targets=train_df['T (degC)'][1:].to_numpy().reshape((-1,1,1)),\n",
        "                                                        sequence_length=1,\n",
        "                                                        batch_size=32,\n",
        "                                                        shuffle=True,\n",
        "                                                        seed=42\n",
        "                                                        )\n",
        "\n",
        "val_ds = tf.keras.utils.timeseries_dataset_from_array(\n",
        "                                                        val_df.to_numpy(),\n",
        "                                                        targets=val_df['T (degC)'][1:].to_numpy().reshape((-1,1,1)),\n",
        "                                                        sequence_length=1,\n",
        "                                                        batch_size=32,\n",
        "                                                        shuffle=True,\n",
        "                                                        seed=42\n",
        "                                                        )\n",
        "\n",
        "test_ds = tf.keras.utils.timeseries_dataset_from_array(\n",
        "                                                        test_df.to_numpy(),\n",
        "                                                        targets=test_df['T (degC)'][1:].to_numpy().reshape((-1,1,1)),\n",
        "                                                        sequence_length=1,\n",
        "                                                        batch_size=32,\n",
        "                                                        shuffle=False,\n",
        "                                                        seed=42\n",
        "                                                        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSKvaLpBFzjj"
      },
      "outputs": [],
      "source": [
        "for example_inputs_, example_labels_ in train_ds.take(1):\n",
        "  print(f'Inputs shape (batch, time, features): {example_inputs_.shape}')\n",
        "  print(f'Labels shape (batch, time, features): {example_labels_.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uceFimLsd299"
      },
      "outputs": [],
      "source": [
        "example_labels_.shape==example_labels.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1bbPiR3VAm_"
      },
      "source": [
        "### Baseline\n",
        "\n",
        "Before building a trainable model it would be good to have a performance baseline as a point for comparison with the later more complicated models.\n",
        "\n",
        "This first task is to predict temperature one hour into the future, given the current value of all features. The current values include the current temperature.\n",
        "\n",
        "So, start with a model that just returns the current temperature as the prediction, predicting \"No change\". This is a reasonable baseline since temperature changes slowly. Of course, this baseline will work less well if you make a prediction further in the future.\n",
        "\n",
        "![Send the input to the output](https://github.com/fabriziobasso/Closet_Index_Tracking/blob/master/images/baseline.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TybQaIsi3yg"
      },
      "outputs": [],
      "source": [
        "class Baseline(tf.keras.Model):\n",
        "  def __init__(self, label_index=None):\n",
        "    super().__init__()\n",
        "    self.label_index = label_index\n",
        "\n",
        "  def call(self, inputs):\n",
        "    if self.label_index is None:\n",
        "      return inputs\n",
        "    result = inputs[:, :, self.label_index]\n",
        "    return result[:, :, tf.newaxis]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vb3f948i8p8"
      },
      "source": [
        "Instantiate and evaluate this model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IS3-QKc4sX0D"
      },
      "outputs": [],
      "source": [
        "baseline = Baseline(label_index=column_indices['T (degC)'])\n",
        "\n",
        "baseline.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
        "                 metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
        "\n",
        "val_performance = {}\n",
        "performance = {}\n",
        "val_performance['Baseline'] = baseline.evaluate(single_step_window.val)\n",
        "performance['Baseline'] = baseline.evaluate(single_step_window.test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWSxpMmilq_Y"
      },
      "outputs": [],
      "source": [
        "val_performance_ = {}\n",
        "performance_ = {}\n",
        "\n",
        "val_performance_['Baseline'] = baseline.evaluate(val_ds)\n",
        "performance_['Baseline'] = baseline.evaluate(test_ds)\n",
        "baseline.evaluate(train_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhBxQcCSs7Ec"
      },
      "source": [
        "That printed some performance metrics, but those don't give you a feeling for how well the model is doing.\n",
        "\n",
        "The `WindowGenerator` has a plot method, but the plots won't be very interesting with only a single sample.\n",
        "\n",
        "So, create a wider `WindowGenerator` that generates windows 24 hours of consecutive inputs and labels at a time. The new `wide_window` variable doesn't change the way the model operates. The model still makes predictions one hour into the future based on a single input time step. Here, the `time` axis acts like the `batch` axis: each prediction is made independently with no interaction between time steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8jNR5uuJ5Zp"
      },
      "outputs": [],
      "source": [
        "wide_window = WindowGenerator(\n",
        "    input_width=24, label_width=24, shift=1,\n",
        "    label_columns=['T (degC)'])\n",
        "\n",
        "wide_window"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAnj7CFZkuYv"
      },
      "source": [
        "This expanded window can be passed directly to the same `baseline` model without any code changes. This is possible because the inputs and labels have the same number of time steps, and the baseline just forwards the input to the output:\n",
        "\n",
        "![One prediction 1h into the future, ever hour.](https://github.com/fabriziobasso/Closet_Index_Tracking/blob/master/images/last_window.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGKdvdg087qs"
      },
      "outputs": [],
      "source": [
        "print('Input shape:', wide_window.example[0].shape)\n",
        "print('Labels shape:', wide_window.example[1].shape)\n",
        "print('Output shape:', baseline(wide_window.example[0]).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKqQHX1K0JW-"
      },
      "source": [
        "By plotting the baseline model's predictions, notice that it is simply the labels shifted right by one hour:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQyAPVLgWTOZ"
      },
      "outputs": [],
      "source": [
        "wide_window.plot(baseline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e93TLUhfAVg2"
      },
      "source": [
        "In the above plots of three examples the single step model is run over the course of 24 hours. This deserves some explanation:\n",
        "\n",
        "- The blue `Inputs` line shows the input temperature at each time step. The model receives all features, this plot only shows the temperature.\n",
        "- The green `Labels` dots show the target prediction value. These dots are shown at the prediction time, not the input time. That is why the range of labels is shifted 1 step relative to the inputs.\n",
        "- The orange `Predictions` crosses are the model's prediction's for each output time step. If the model were predicting perfectly the predictions would land directly on the `Labels`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4aOJScj52Yu"
      },
      "source": [
        "### Linear model\n",
        "\n",
        "The simplest **trainable** model you can apply to this task is to insert linear transformation between the input and output. In this case the output from a time step only depends on that step:\n",
        "\n",
        "![A single step prediction](https://github.com/fabriziobasso/Closet_Index_Tracking/blob/master/images/narrow_window.png?raw=1)\n",
        "\n",
        "A `tf.keras.layers.Dense` layer with no `activation` set is a linear model. The layer only transforms the last axis of the data from `(batch, time, inputs)` to `(batch, time, units)`; it is applied independently to every item across the `batch` and `time` axes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdkzRcxz5D8I"
      },
      "outputs": [],
      "source": [
        "linear = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "linear_ = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(units=1)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGPtD8Ev5D41"
      },
      "outputs": [],
      "source": [
        "print('Input shape:', single_step_window.example[0].shape)\n",
        "print('Label shape:', single_step_window.example[1].shape)\n",
        "print('Output shape:', linear(single_step_window.example[0]).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGFiGVGJaa7x"
      },
      "outputs": [],
      "source": [
        "for example_inputs_, example_labels_ in train_ds.take(1):\n",
        "  print('Input shape:', example_inputs_.shape)\n",
        "  print('Label shape:', example_labels_.shape)\n",
        "  print('Output shape:', linear(example_inputs_).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMZTYIj3bYLg"
      },
      "source": [
        "This tutorial trains many models, so package the training procedure into a function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbCL6VIrk-Gt"
      },
      "outputs": [],
      "source": [
        "MAX_EPOCHS = 25\n",
        "\n",
        "def compile_and_fit(model, window, patience=2):\n",
        "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                    patience=patience,\n",
        "                                                    mode='min')\n",
        "\n",
        "  model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
        "\n",
        "  history = model.fit(window.train, epochs=MAX_EPOCHS,\n",
        "                      validation_data=window.val,\n",
        "                      callbacks=[early_stopping])\n",
        "  return history\n",
        "\n",
        "\n",
        "def compile_and_fit_v0(model, train_ds=train_ds, val_ds=val_ds, patience=2):\n",
        "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                    patience=patience,\n",
        "                                                    mode='min')\n",
        "\n",
        "  model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
        "\n",
        "  history = model.fit(train_ds, epochs=MAX_EPOCHS,\n",
        "                      validation_data=val_ds,\n",
        "                      callbacks=[early_stopping])\n",
        "  return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OobVjM-schwj"
      },
      "source": [
        "Train the model and evaluate its performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9agbz2qB9bLS"
      },
      "outputs": [],
      "source": [
        "#history = compile_and_fit(linear, single_step_window)\n",
        "\n",
        "#linear.save(folders_nn+f'linear_model')\n",
        "linear = tf.keras.models.load_model(folders_nn+f'linear_model')\n",
        "\n",
        "val_performance['Linear'] = linear.evaluate(single_step_window.val)\n",
        "performance['Linear'] = linear.evaluate(single_step_window.test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cdq2rYe2p1Le"
      },
      "outputs": [],
      "source": [
        "#history = compile_and_fit_v0(linear_)\n",
        "\n",
        "#linear_.save(folders_nn+f'linear_model_v0')\n",
        "linear_ = tf.keras.models.load_model(folders_nn+f'linear_model_v0')\n",
        "\n",
        "val_performance_['Linear'] = linear_.evaluate(val_ds, verbose=0)\n",
        "performance_['Linear'] = linear_.evaluate(test_ds, verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U9XukYh8beN"
      },
      "source": [
        "Like the `baseline` model, the linear model can be called on batches of wide windows. Used this way the model makes a set of independent predictions on consecutive time steps. The `time` axis acts like another `batch` axis. There are no interactions between the predictions at each time step.\n",
        "\n",
        "![A single step prediction](https://github.com/fabriziobasso/Closet_Index_Tracking/blob/master/images/wide_window.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9UVM5Sw9KQN"
      },
      "outputs": [],
      "source": [
        "print('Input shape:', single_step_window.example[0].shape)\n",
        "print('Output shape:', linear(single_step_window.example[0]).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwmVMKDBsMh6"
      },
      "source": [
        "Like the `baseline` model, the linear model can be called on batches of wide windows. Used this way the model makes a set of independent predictions on consecutive time steps. The `time` axis acts like another `batch` axis. There are no interactions between the predictions at each time step.\n",
        "\n",
        "![A single step prediction](https://github.com/fabriziobasso/Closet_Index_Tracking/blob/master/images/wide_window.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKmrc9VvsMh6"
      },
      "outputs": [],
      "source": [
        "print('Input shape:', wide_window.example[0].shape)\n",
        "print('Output shape:', linear_(wide_window.example[0]).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-CGj85oKaOG"
      },
      "source": [
        "Here is the plot of its example predictions on the `wide_window`, note how in many cases the prediction is clearly better than just returning the input temperature, but in a few cases it's worse:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is51vU8EMl6c"
      },
      "source": [
        "One advantage to linear models is that they're relatively simple to  interpret.\n",
        "You can pull out the layer's weights and visualize the weight assigned to each input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCC8VVo-OvwV"
      },
      "outputs": [],
      "source": [
        "plt.bar(x = range(len(train_df.columns)),\n",
        "        height=linear.layers[0].kernel[:,0].numpy())\n",
        "axis = plt.gca()\n",
        "axis.set_xticks(range(len(train_df.columns)))\n",
        "_ = axis.set_xticklabels(train_df.columns, rotation=90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEWRjZi1nufo"
      },
      "outputs": [],
      "source": [
        "plt.bar(x = range(len(train_df.columns)),\n",
        "        height=linear_.layers[0].kernel[:,0].numpy())\n",
        "axis = plt.gca()\n",
        "axis.set_xticks(range(len(train_df.columns)))\n",
        "_ = axis.set_xticklabels(train_df.columns, rotation=90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ylng7215boIY"
      },
      "source": [
        "Sometimes the model doesn't even place the most weight on the input `T (degC)`. This is one of the risks of random initialization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W18e6da1cNbw"
      },
      "source": [
        "### Dense\n",
        "\n",
        "Before applying models that actually operate on multiple time-steps, it's worth checking the performance of deeper, more powerful, single input step models.\n",
        "\n",
        "Here's a model similar to the `linear` model, except it stacks several a few `Dense` layers between the input and the output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHiJqAuOnucG"
      },
      "outputs": [],
      "source": [
        "dense = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "#history = compile_and_fit(dense, single_step_window)\n",
        "\n",
        "#dense.save(folders_nn+f'dense')\n",
        "dense = tf.keras.models.load_model(folders_nn+f'dense')\n",
        "\n",
        "val_performance['Dense'] = dense.evaluate(single_step_window.val)\n",
        "performance['Dense'] = dense.evaluate(single_step_window.test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMfcfPa3nuY-"
      },
      "outputs": [],
      "source": [
        "dense_ = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "#history = compile_and_fit_v0(dense_)\n",
        "\n",
        "#dense_.save(folders_nn+f'dense_')\n",
        "dense_ = tf.keras.models.load_model(folders_nn+f'dense_')\n",
        "\n",
        "val_performance_['Dense'] = dense_.evaluate(val_ds)\n",
        "performance_['Dense'] = dense_.evaluate(test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOk-lOahnuDX"
      },
      "outputs": [],
      "source": [
        "val_performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BphcaxK9tguj"
      },
      "outputs": [],
      "source": [
        "val_performance_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5dv_whJdswH"
      },
      "source": [
        "### Multi-step dense\n",
        "\n",
        "A single-time-step model has no context for the current values of its inputs. It can't see how the input features are changing over time. To address this issue the model needs access to multiple time steps when making predictions:\n",
        "\n",
        "[Three time steps are used for each prediction.](https://drive.google.com/file/d/1Jyy2bwbyay2EVvlVOPoK8V95npcXbUUA/view?usp=drive_link)\n",
        "\n",
        "<img src=\"https://drive.google.com/file/d/1PbOzhWrEdJcm8KEt0vLzBhSNzfa-Mfeb/view?usp=drive_link\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zac-ti8agbJ7"
      },
      "source": [
        "The `baseline`, `linear` and `dense` models handled each time step independently. Here the model will take multiple time steps as input to produce a single output.\n",
        "\n",
        "Create a `WindowGenerator` that will produce batches of three-hour inputs and one-hour labels:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtN4BwZ37niR"
      },
      "source": [
        "Note that the `Window`'s `shift` parameter is relative to the end of the two windows.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdiSWPGPtgrR"
      },
      "outputs": [],
      "source": [
        "CONV_WIDTH = 3\n",
        "conv_window = WindowGenerator(\n",
        "    input_width=CONV_WIDTH,\n",
        "    label_width=1,\n",
        "    shift=1,\n",
        "    label_columns=['T (degC)'])\n",
        "\n",
        "conv_window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERpGxwyntgny"
      },
      "outputs": [],
      "source": [
        "conv_window.plot()\n",
        "plt.title(\"Given 3 hours of inputs, predict 1 hour into the future.\", y=3.5);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctyR7yRetgkM"
      },
      "outputs": [],
      "source": [
        "print('Input shape:', conv_window.example[0].shape)\n",
        "print('Output shape:', conv_window.example[1].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLX-UUmo350w"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # extra code – ensures reproducibility\n",
        "seq_lenght=3\n",
        "CONV_WIDTH_=1\n",
        "\n",
        "def split_inputs_and_targets(mulvar_series, ahead=CONV_WIDTH_, target_col=1):\n",
        "    return mulvar_series[:, :-ahead], tf.reshape(mulvar_series[:, -ahead:, target_col],(-1,CONV_WIDTH_,1))\n",
        "\n",
        "ahead_train_ds = tf.keras.utils.timeseries_dataset_from_array(\n",
        "    train_df.to_numpy(),\n",
        "    targets=None,\n",
        "    sequence_length=seq_lenght + CONV_WIDTH_,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    seed=42).map(split_inputs_and_targets)\n",
        "\n",
        "ahead_valid_ds = tf.keras.utils.timeseries_dataset_from_array(\n",
        "    val_df.to_numpy(),\n",
        "    targets=None,\n",
        "    sequence_length=seq_lenght + CONV_WIDTH_,\n",
        "    batch_size=32).map(split_inputs_and_targets)\n",
        "\n",
        "ahead_test_ds = tf.keras.utils.timeseries_dataset_from_array(\n",
        "    test_df.to_numpy(),\n",
        "    targets=None,\n",
        "    sequence_length=seq_lenght + CONV_WIDTH_,batch_size=32).map(split_inputs_and_targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SI-q_8wqyp1E"
      },
      "outputs": [],
      "source": [
        "for example_inputs_, example_labels_ in ahead_train_ds.take(1):\n",
        "  print(f'Inputs shape (batch, time, features): {example_inputs_.shape}')\n",
        "  print(f'Labels shape (batch, time, features): {example_labels_.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDJc9Ilmypx9"
      },
      "outputs": [],
      "source": [
        "#example_inputs_[:,:,1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKcgu5bj66KF"
      },
      "outputs": [],
      "source": [
        "#example_labels_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rZpfUSl7qGp"
      },
      "outputs": [],
      "source": [
        "#train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We0HdMxKeqB_"
      },
      "source": [
        "You could train a `dense` model on a multiple-input-step window by adding a `tf.keras.layers.Flatten` as the first layer of the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNQnUOkOnC1G"
      },
      "outputs": [],
      "source": [
        "multi_step_dense = tf.keras.Sequential([\n",
        "    # Shape: (time, features) => (time*features)\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(units=32, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=32, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=1),\n",
        "    # Add back the time dimension.\n",
        "    # Shape: (outputs) => (1, outputs)\n",
        "    tf.keras.layers.Reshape([1, -1]),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cayD74luo4Vq"
      },
      "outputs": [],
      "source": [
        "print('Input shape:', conv_window.example[0].shape)\n",
        "print('Output shape:', multi_step_dense(conv_window.example[0]).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fu91yEbRo9-J"
      },
      "outputs": [],
      "source": [
        "#history = compile_and_fit(multi_step_dense, conv_window)\n",
        "\n",
        "#multi_step_dense.save(folders_nn+f'multi_step_dense')\n",
        "multi_step_dense = tf.keras.models.load_model(folders_nn+f'multi_step_dense')\n",
        "\n",
        "IPython.display.clear_output()\n",
        "val_performance['Multi step dense'] = multi_step_dense.evaluate(conv_window.val)\n",
        "performance['Multi step dense'] = multi_step_dense.evaluate(conv_window.test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnqdXYT6pkEh"
      },
      "outputs": [],
      "source": [
        "conv_window.plot(multi_step_dense)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOXJpqyMFxAZ"
      },
      "outputs": [],
      "source": [
        "multi_step_dense_v0 = tf.keras.Sequential([\n",
        "    # Shape: (time, features) => (time*features)\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(units=32, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=32, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=1),\n",
        "    # Add back the time dimension.\n",
        "      # Shape: (outputs) => (1, outputs)\n",
        "      tf.keras.layers.Reshape([1, -1]),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4g_TlBt8FRQl"
      },
      "outputs": [],
      "source": [
        "for example_inputs_, example_labels_ in ahead_train_ds.take(1):\n",
        "\n",
        "  print('Input shape:', example_inputs_.shape)\n",
        "  print('Label shape:', example_labels_.shape)\n",
        "  print('Output shape:', multi_step_dense_v0(example_inputs_).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAlzCvWdyprM"
      },
      "outputs": [],
      "source": [
        "#history = compile_and_fit_v0(multi_step_dense_v0, train_ds=ahead_train_ds, val_ds=ahead_valid_ds,)\n",
        "\n",
        "#multi_step_dense_v0.save(folders_nn+f'multi_step_dense_v0')\n",
        "multi_step_dense_v0 = tf.keras.models.load_model(folders_nn+f'multi_step_dense_v0')\n",
        "\n",
        "IPython.display.clear_output()\n",
        "val_performance_['Multi step dense'] = multi_step_dense_v0.evaluate(ahead_valid_ds)\n",
        "performance_['Multi step dense'] = multi_step_dense_v0.evaluate(ahead_test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWfrsP8mq8lV"
      },
      "source": [
        "The main down-side of this approach is that the resulting model can only be executed on input windows of exactly this shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-q6tz5Yq8Jk"
      },
      "outputs": [],
      "source": [
        "print('Input shape:', wide_window.example[0].shape)\n",
        "try:\n",
        "  print('Output shape:', multi_step_dense(wide_window.example[0]).shape)\n",
        "except Exception as e:\n",
        "  print(f'\\n{type(e).__name__}:{e}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvvajm3ip_8V"
      },
      "source": [
        "The convolutional models in the next section fix this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrpU6gwSJome"
      },
      "source": [
        "### Convolution neural network\n",
        "\n",
        "A convolution layer (`tf.keras.layers.Conv1D`) also takes multiple time steps as input to each prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdLBwoaHmsWb"
      },
      "source": [
        "Below is the **same** model as `multi_step_dense`, re-written with a convolution.\n",
        "\n",
        "Note the changes:\n",
        "* The `tf.keras.layers.Flatten` and the first `tf.keras.layers.Dense` are replaced by a `tf.keras.layers.Conv1D`.\n",
        "* The `tf.keras.layers.Reshape` is no longer necessary since the convolution keeps the time axis in its output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5azaMBj4ac9t"
      },
      "outputs": [],
      "source": [
        "conv_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv1D(filters=32,\n",
        "                           kernel_size=(CONV_WIDTH,),\n",
        "                           activation='relu', name=\"conv_1\"),\n",
        "    tf.keras.layers.Dense(units=32, activation='relu',name=\"dense_1\"),\n",
        "    tf.keras.layers.Dense(units=1,name=\"output_layer\"),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftaH6B5ECRiK"
      },
      "source": [
        "Run it on an example batch to check that the model produces outputs with the expected shape:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHYJmsoVypnH"
      },
      "outputs": [],
      "source": [
        "print(\"Conv model on `conv_window`\")\n",
        "print('Input shape:', conv_window.example[0].shape)\n",
        "print('Output shape:', conv_window.example[1].shape)\n",
        "print('Output shape Model:', conv_model(conv_window.example[0]).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2kUB7ywFN6o"
      },
      "outputs": [],
      "source": [
        "conv_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5m4kC-jGCY3x"
      },
      "source": [
        "Train and evaluate it on the ` conv_window` and it should give performance similar to the `multi_step_dense` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDVWdm4paUW7"
      },
      "outputs": [],
      "source": [
        "history = compile_and_fit(conv_model, conv_window)\n",
        "\n",
        "#conv_model.save(folders_nn+f'conv_model_v0')\n",
        "#conv_model = tf.keras.models.load_model(folders_nn+f'conv_model_v0')\n",
        "\n",
        "IPython.display.clear_output()\n",
        "val_performance['Conv'] = conv_model.evaluate(conv_window.val)\n",
        "performance['Conv'] = conv_model.evaluate(conv_window.test, verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYRipDeXs0Kr"
      },
      "source": [
        "The difference between this `conv_model` and the `multi_step_dense` model is that the `conv_model` can be run on inputs of any length. The convolutional layer is applied to a sliding window of inputs:\n",
        "\n",
        "[Executing a convolutional model on a sequence](https://drive.google.com/file/d/18YTMppzoB_qMT4D0WwgxDvKu2P5O6xUw/view?usp=sharing)\n",
        "\n",
        "If you run it on wider input, it produces wider output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoqccxx9r5jF"
      },
      "outputs": [],
      "source": [
        "print(\"Wide window\")\n",
        "print('Input shape:', wide_window.example[0].shape)\n",
        "print('Labels shape:', wide_window.example[1].shape)\n",
        "print('Output shape:', conv_model(wide_window.example[0]).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvKrst1x67MZ"
      },
      "outputs": [],
      "source": [
        "conv_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_WGxtLIHhRF"
      },
      "source": [
        "Note that the output is shorter than the input. To make training or plotting work, you need the labels, and prediction to have the same length. So build a `WindowGenerator` to produce wide windows with a few extra input time steps so the label and prediction lengths match:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VPvJ_VwTc0f"
      },
      "outputs": [],
      "source": [
        "LABEL_WIDTH = 24\n",
        "INPUT_WIDTH = LABEL_WIDTH + (CONV_WIDTH - 1)\n",
        "wide_conv_window = WindowGenerator(\n",
        "    input_width=INPUT_WIDTH,\n",
        "    label_width=LABEL_WIDTH,\n",
        "    shift=1,\n",
        "    label_columns=['T (degC)'])\n",
        "\n",
        "wide_conv_window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtqlWYXeKXej"
      },
      "outputs": [],
      "source": [
        "print(\"Wide conv window\")\n",
        "print('Input shape:', wide_conv_window.example[0].shape)\n",
        "print('Labels shape:', wide_conv_window.example[1].shape)\n",
        "print('Output shape:', conv_model(wide_conv_window.example[0]).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aYBGEy6hNc7"
      },
      "outputs": [],
      "source": [
        "wide_conv_window.plot(conv_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4crpOcoMlSe"
      },
      "source": [
        "### Recurrent neural network\n",
        "\n",
        "A Recurrent Neural Network (RNN) is a type of neural network well-suited to time series data. RNNs process a time series step-by-step, maintaining an internal state from time-step to time-step.\n",
        "\n",
        "You can learn more in the [Text generation with an RNN](https://www.tensorflow.org/text/tutorials/text_generation) tutorial and the [Recurrent Neural Networks (RNN) with Keras](https://www.tensorflow.org/guide/keras/rnn) guide.\n",
        "\n",
        "In this tutorial, you will use an RNN layer called Long Short-Term Memory (`tf.keras.layers.LSTM`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfQbHSMb1ATa"
      },
      "source": [
        "An important constructor argument for all Keras RNN layers, such as `tf.keras.layers.LSTM`, is the `return_sequences` argument. This setting can configure the layer in one of two ways:\n",
        "\n",
        "1. If `False`, the default, the layer only returns the output of the final time step, giving the model time to warm up its internal state before making a single prediction:\n",
        "\n",
        "[An LSTM warming up and making a single prediction](https://drive.google.com/file/d/1bteQhUHj-xuS203ruHTY5mUhbsSY52Ct/view?usp=sharing)\n",
        "\n",
        "2. If `True`, the layer returns an output for each input. This is useful for:\n",
        "  * Stacking RNN layers.\n",
        "  * Training a model on multiple time steps simultaneously.\n",
        "\n",
        "[An LSTM making a prediction after every time step](https://drive.google.com/file/d/1PbOzhWrEdJcm8KEt0vLzBhSNzfa-Mfeb/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvOqFKijhNV6"
      },
      "outputs": [],
      "source": [
        "lstm_model = tf.keras.models.Sequential([\n",
        "    # Shape [batch, time, features] => [batch, time, lstm_units]\n",
        "    tf.keras.layers.LSTM(32, return_sequences=True),\n",
        "    # Shape => [batch, time, features]\n",
        "    tf.keras.layers.Dense(units=1)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F124B00KZcLC"
      },
      "source": [
        "With `return_sequences=True`, the model can be trained on 24 hours of data at a time.\n",
        "\n",
        "Note: This will give a pessimistic view of the model's performance. On the first time step, the model has no access to previous steps and, therefore, can't do any better than the simple `linear` and `dense` models shown earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZEROCQVYV6q"
      },
      "outputs": [],
      "source": [
        "print('Input shape:', wide_window.example[0].shape)\n",
        "print('Labels shape:', wide_window.example[1].shape)\n",
        "print('Output shape:', lstm_model(wide_window.example[0]).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fo6kwkahNRH"
      },
      "outputs": [],
      "source": [
        "lstm_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71Z_0pzhBmx1"
      },
      "outputs": [],
      "source": [
        "lstm_model_ = tf.keras.models.Sequential([\n",
        "    # Shape [batch, time, features] => [batch, time, lstm_units]\n",
        "    tf.keras.layers.LSTM(32, return_sequences=True),\n",
        "    # Shape => [batch, time, features]\n",
        "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(units=1))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0Iw7p4267Pv"
      },
      "outputs": [],
      "source": [
        "print('Input shape:', wide_window.example[0].shape)\n",
        "print('Labels shape:', wide_window.example[1].shape)\n",
        "print('Output shape:', lstm_model_(wide_window.example[0]).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQLSaPQK67I_"
      },
      "outputs": [],
      "source": [
        "lstm_model_.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTlQhqDRypiI"
      },
      "outputs": [],
      "source": [
        "#history = compile_and_fit(lstm_model, wide_window)\n",
        "\n",
        "#lstm_model.save(folders_nn+f'lstm_model_v0')\n",
        "lstm_model = tf.keras.models.load_model(folders_nn+f'lstm_model_v0')\n",
        "\n",
        "IPython.display.clear_output()\n",
        "val_performance['LSTM'] = lstm_model.evaluate(wide_window.val)\n",
        "performance['LSTM'] = lstm_model.evaluate(wide_window.test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajPhdke1ypcC"
      },
      "outputs": [],
      "source": [
        "wide_window.plot(lstm_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkfKhlpCeQx1"
      },
      "source": [
        "### More Complex Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR3cTThiqRI-"
      },
      "source": [
        "#### - Multi Layer Conv1D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dSnK-0dRoU0"
      },
      "outputs": [],
      "source": [
        "# MODEL V0\n",
        "inputs = tf.keras.Input(shape=(24,21))\n",
        "\n",
        "xn=tf.keras.layers.LayerNormalization(epsilon=0.001)(inputs)\n",
        "x0 = tf.keras.layers.Conv1D(filters=32, kernel_size=(3,),activation='relu',padding=\"same\")(xn)\n",
        "x0 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(units=32, activation='relu'))(x0)\n",
        "\n",
        "x1 = tf.keras.layers.AveragePooling1D(pool_size=2, strides=1, padding=\"same\")(x0)\n",
        "x1 = tf.keras.layers.Conv1D(filters=32, kernel_size=(3,),activation='relu',padding=\"same\")(x1)\n",
        "x1 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(units=32, activation='relu'))(x1)\n",
        "\n",
        "x3 = tf.keras.layers.Concatenate(axis=-1)([x0,x1])\n",
        "x3 = tf.keras.layers.Dense(units=32, activation='relu')(x3)\n",
        "\n",
        "output = tf.keras.layers.Dense(units=1)(x3)\n",
        "\n",
        "model_v0 = tf.keras.Model(inputs=inputs, outputs=output)\n",
        "\n",
        "# MODEL V1\n",
        "inputs = tf.keras.Input(shape=(24,21))\n",
        "\n",
        "xn=tf.keras.layers.LayerNormalization(epsilon=0.001)(inputs)\n",
        "x0 = tf.keras.layers.Conv1D(filters=32, kernel_size=(3,),activation='relu',padding=\"same\")(xn)\n",
        "\n",
        "x1 = tf.keras.layers.AveragePooling1D(pool_size=2, strides=1, padding=\"same\")(xn)\n",
        "x1 = tf.keras.layers.Conv1D(filters=32, kernel_size=(3,),activation='relu',padding=\"same\")(x1)\n",
        "\n",
        "x3 = tf.keras.layers.Concatenate(axis=-1)([x0,x1])\n",
        "x3 = tf.keras.layers.Dense(units=32, activation='relu')(x3)\n",
        "\n",
        "output = tf.keras.layers.Dense(units=1)(x3)\n",
        "\n",
        "model_v1 = tf.keras.Model(inputs=inputs, outputs=output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zlJw3lSSUT7"
      },
      "outputs": [],
      "source": [
        "model_v0.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74zA25KftkSM"
      },
      "outputs": [],
      "source": [
        "model_v1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rp0zL9AKVgnw"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(model_v0,show_shapes=True,show_layer_activations=False,rankdir='LR',expand_nested=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pDCbmV4ShMr"
      },
      "outputs": [],
      "source": [
        "print('Input shape:', wide_window.example[0].shape)\n",
        "print('Labels shape:', wide_window.example[1].shape)\n",
        "print('Output shape:', model_v0(wide_window.example[0]).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gX70sEksaJo0"
      },
      "outputs": [],
      "source": [
        "#history = compile_and_fit(model_v0, wide_window)\n",
        "\n",
        "#model_v0.save(folders_nn+f'complex_v0')\n",
        "model_v0 = tf.keras.models.load_model(folders_nn+f'complex_v0')\n",
        "\n",
        "IPython.display.clear_output()\n",
        "val_performance['Conv_v0'] = model_v0.evaluate(wide_window.val)\n",
        "performance['Conv_v0'] = model_v0.evaluate(wide_window.test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-PR3inpt1Q0"
      },
      "outputs": [],
      "source": [
        "#history = compile_and_fit(model_v1, wide_window)\n",
        "\n",
        "#model_v1.save(folders_nn+f'complex_v1')\n",
        "model_v1 = tf.keras.models.load_model(folders_nn+f'complex_v1')\n",
        "\n",
        "IPython.display.clear_output()\n",
        "val_performance['Conv_v1'] = model_v1.evaluate(wide_window.val)\n",
        "performance['Conv_v1'] = model_v1.evaluate(wide_window.test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIa8T4P-ad-r"
      },
      "outputs": [],
      "source": [
        "wide_window.plot(model_v0, max_subplots=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Jvdvz4cqcrU"
      },
      "source": [
        "#### - Multi Layer LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OyNVQA8qmqF"
      },
      "outputs": [],
      "source": [
        "inputs = tf.keras.Input(shape=(24,21))\n",
        "\n",
        "xn=tf.keras.layers.LayerNormalization(epsilon=0.001)(inputs)\n",
        "x0 = tf.keras.layers.LSTM(32, return_sequences=True)(xn)\n",
        "\n",
        "x1 = tf.keras.layers.AveragePooling1D(pool_size=2, strides=1, padding=\"same\")(xn)\n",
        "x1 = tf.keras.layers.LSTM(32, return_sequences=True)(x1)\n",
        "\n",
        "x3 = tf.keras.layers.Concatenate(axis=-1)([x0,x1])\n",
        "x3 = tf.keras.layers.Dense(units=32, activation='relu')(x3)\n",
        "\n",
        "output = tf.keras.layers.Dense(units=1)(x3)\n",
        "\n",
        "model_v2 = tf.keras.Model(inputs=inputs, outputs=output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfqmhXmMqmm1"
      },
      "outputs": [],
      "source": [
        "model_v2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "596HdHWwqmjM"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(model_v2,show_shapes=True,show_layer_activations=False,rankdir='LR',expand_nested=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pF_d7v1NNzto"
      },
      "outputs": [],
      "source": [
        "#history = compile_and_fit(model_v2, wide_window)\n",
        "\n",
        "#model_v2.save(folders_nn+f'complex_v3')\n",
        "model_v2 = tf.keras.models.load_model(folders_nn+f'complex_v3')\n",
        "\n",
        "IPython.display.clear_output()\n",
        "val_performance['LSTM_v0'] = model_v2.evaluate(wide_window.val)\n",
        "performance['LSTM_v0'] = model_v2.evaluate(wide_window.test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGjcJsAQJUkI"
      },
      "source": [
        "### Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sODAwr2ndtDB"
      },
      "source": [
        "There are clearly diminishing returns as a function of model complexity on this problem:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZwWBA8S6B3L"
      },
      "outputs": [],
      "source": [
        "x = np.arange(len(performance))\n",
        "width = 0.3\n",
        "metric_name = 'mean_absolute_error'\n",
        "metric_index = lstm_model.metrics_names.index('mean_absolute_error')\n",
        "val_mae = [v[metric_index] for v in val_performance.values()]\n",
        "test_mae = [v[metric_index] for v in performance.values()]\n",
        "\n",
        "plt.ylabel('mean_absolute_error [T (degC), normalized]')\n",
        "plt.bar(x - 0.17, val_mae, width, label='Validation')\n",
        "plt.bar(x + 0.17, test_mae, width, label='Test')\n",
        "plt.xticks(ticks=x, labels=performance.keys(),\n",
        "           rotation=45)\n",
        "plt.grid(linestyle='--', alpha=0.5)\n",
        "_ = plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq3hUsedCEmJ"
      },
      "source": [
        "The metrics for the multi-output models in the first half of this tutorial show the performance averaged across all output features. These performances are similar but also averaged across output time steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKq3eAIvH4Db"
      },
      "outputs": [],
      "source": [
        "for name, value in performance.items():\n",
        "  print(f'{name:20s}: {value[1]:0.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpBFwfnaHP23"
      },
      "source": [
        "The gains achieved going from a dense model to convolutional and recurrent models are only a few percent (if any), and the autoregressive model performed clearly worse. So these more complex approaches may not be worth while on **this** problem, but there was no way to know without trying, and these models could be helpful for **your** problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5rUJ_2YMWzG"
      },
      "source": [
        "### Multi-output models\n",
        "\n",
        "The models so far all predicted a single output feature, `T (degC)`, for a single time step.\n",
        "\n",
        "All of these models can be converted to predict multiple features just by changing the number of units in the output layer and adjusting the training windows to include all features in the `labels` (`example_labels`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Gk0Z91xjOwv"
      },
      "outputs": [],
      "source": [
        "single_step_window = WindowGenerator(\n",
        "    # `WindowGenerator` returns all features as labels if you\n",
        "    # don't set the `label_columns` argument.\n",
        "    input_width=1, label_width=1, shift=1)\n",
        "\n",
        "wide_window = WindowGenerator(\n",
        "    input_width=24, label_width=24, shift=1)\n",
        "\n",
        "for example_inputs, example_labels in wide_window.train.take(1):\n",
        "  print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n",
        "  print(f'Labels shape (batch, time, features): {example_labels.shape}\\n')\n",
        "\n",
        "\n",
        "for example_inputs, example_labels in single_step_window.train.take(1):\n",
        "  print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n",
        "  print(f'Labels shape (batch, time, features): {example_labels.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmcjHfDskX1N"
      },
      "source": [
        "Note above that the `features` axis of the labels now has the same depth as the inputs, instead of `1`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k7S5IHNhSNF"
      },
      "source": [
        "#### Baseline\n",
        "\n",
        "The same baseline model (`Baseline`) can be used here, but this time repeating all features instead of selecting a specific `label_index`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqqB9W-pjr5i"
      },
      "outputs": [],
      "source": [
        "baseline = Baseline()\n",
        "baseline.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
        "                 metrics=[tf.keras.metrics.MeanAbsoluteError()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltQdgaqQjQWu"
      },
      "outputs": [],
      "source": [
        "val_performance = {}\n",
        "performance = {}\n",
        "val_performance['Baseline'] = baseline.evaluate(wide_window.val)\n",
        "performance['Baseline'] = baseline.evaluate(wide_window.test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfbCrf5q3P6n"
      },
      "source": [
        "#### Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdpzH1dYjdIN"
      },
      "outputs": [],
      "source": [
        "dense = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(units=128, activation='relu',name=\"dense_0\"),\n",
        "    tf.keras.layers.Dense(units=64, activation='relu',name=\"dense_1\"),\n",
        "    tf.keras.layers.Dense(units=num_features,name=\"output\")\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vM0om_YW05s"
      },
      "outputs": [],
      "source": [
        "print('Input shape:', wide_window.example[0].shape)\n",
        "print('Labels shape:', wide_window.example[1].shape)\n",
        "print('Output shape:', dense(wide_window.example[0]).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29vs-M1HWvf1"
      },
      "outputs": [],
      "source": [
        "dense.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uHuU9Cd3PTo"
      },
      "outputs": [],
      "source": [
        "#history = compile_and_fit(dense, single_step_window)\n",
        "\n",
        "#dense.save(folders_nn+f'dense_mo')\n",
        "dense = tf.keras.models.load_model(folders_nn+f'dense_mo')\n",
        "\n",
        "IPython.display.clear_output()\n",
        "val_performance['Dense'] = dense.evaluate(single_step_window.val)\n",
        "performance['Dense'] = dense.evaluate(single_step_window.test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsc9pur_mHsx"
      },
      "source": [
        "#### RNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QbGLMyomXaz"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "wide_window = WindowGenerator(\n",
        "    input_width=24, label_width=24, shift=1)\n",
        "\n",
        "lstm_model = tf.keras.models.Sequential([\n",
        "    # Shape [batch, time, features] => [batch, time, lstm_units]\n",
        "    tf.keras.layers.LSTM(32, return_sequences=True),\n",
        "    tf.keras.layers.LSTM(32, return_sequences=True),\n",
        "    # Shape => [batch, time, features]\n",
        "    tf.keras.layers.Dense(units=num_features)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84yR8rWuZqkE"
      },
      "outputs": [],
      "source": [
        "print('Input shape:', wide_window.example[0].shape)\n",
        "print('Labels shape:', wide_window.example[1].shape)\n",
        "print('Output shape:', lstm_model(wide_window.example[0]).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxNu04hXZqZe"
      },
      "outputs": [],
      "source": [
        "lstm_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqzjAGTOZ2Og"
      },
      "outputs": [],
      "source": [
        "#history = compile_and_fit(lstm_model, wide_window)\n",
        "\n",
        "#lstm_model.save(folders_nn+f'lstm_model_mo')\n",
        "lstm_model = tf.keras.models.load_model(folders_nn+f'lstm_model_mo')\n",
        "\n",
        "IPython.display.clear_output()\n",
        "val_performance['LSTM'] = lstm_model.evaluate( wide_window.val)\n",
        "performance['LSTM'] = lstm_model.evaluate( wide_window.test)\n",
        "\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crSp3mVWUX5N"
      },
      "source": [
        "<a id=\"residual\"></a>\n",
        "\n",
        "#### Advanced: Residual connections\n",
        "\n",
        "The `Baseline` model from earlier took advantage of the fact that the sequence doesn't change drastically from time step to time step. Every model trained in this tutorial so far was randomly initialized, and then had to learn that the output is a a small change from the previous time step.\n",
        "\n",
        "While you can get around this issue with careful initialization, it's  simpler to build this into the model structure.\n",
        "\n",
        "It's common in time series analysis to build models that instead of predicting the next value, predict how the value will change in the next time step. Similarly, <a href=\"https://arxiv.org/abs/1512.03385\" class=\"external\">residual networks</a>—or ResNets—in deep learning refer to architectures where each layer adds to the model's accumulating result.\n",
        "\n",
        "That is how you take advantage of the knowledge that the change should be small.\n",
        "\n",
        "[A model with a residual connection](https://drive.google.com/file/d/1XwyLn92R-3GT7F33MxWwmLNb9chqcqGR/view?usp=sharing)\n",
        "\n",
        "Essentially, this initializes the model to match the `Baseline`. For this task it helps models converge faster, with slightly better performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVX9iq-IUX5N"
      },
      "source": [
        "This approach can be used in conjunction with any model discussed in this tutorial.\n",
        "\n",
        "Here, it is being applied to the LSTM model, note the use of the `tf.initializers.zeros` to ensure that the initial predicted changes are small, and don't overpower the residual connection. There are no symmetry-breaking concerns for the gradients here, since the `zeros` are only used on the last layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQJso5fTUX5O"
      },
      "outputs": [],
      "source": [
        "class ResidualWrapper(tf.keras.Model):\n",
        "  def __init__(self, model):\n",
        "    super().__init__()\n",
        "    self.model = model\n",
        "\n",
        "  def call(self, inputs, *args, **kwargs):\n",
        "    delta = self.model(inputs, *args, **kwargs)\n",
        "\n",
        "    # The prediction for each time step is the input\n",
        "    # from the previous time step plus the delta\n",
        "    # calculated by the model.\n",
        "    return inputs + delta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VuAD-mBUX5O"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "residual_lstm = tf.keras.Sequential([ResidualWrapper(\n",
        "                                      tf.keras.Sequential([\n",
        "                                      tf.keras.layers.LSTM(32, return_sequences=True),\n",
        "                                      tf.keras.layers.Dense(\n",
        "                                          num_features,\n",
        "                                          # The predicted deltas should start small.\n",
        "                                          # Therefore, initialize the output layer with zeros.\n",
        "                                          kernel_initializer=tf.initializers.zeros())])),\n",
        "                                      tf.keras.layers.LSTM(32, return_sequences=True),\n",
        "                                      tf.keras.layers.Dense(num_features)\n",
        "\n",
        "                                    ])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Input shape:', wide_window.example[0].shape)\n",
        "print('Labels shape:', wide_window.example[1].shape)\n",
        "print('Output shape:', residual_lstm(wide_window.example[0]).shape)"
      ],
      "metadata": {
        "id": "UqOtqRAsUX5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "residual_lstm.summary(expand_nested=True)"
      ],
      "metadata": {
        "id": "Pj4p6cptUX5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = compile_and_fit(residual_lstm, wide_window)\n",
        "\n",
        "residual_lstm.save(folders_nn+f'risidual_lstm_mo')\n",
        "residual_lstm = tf.keras.models.load_model(folders_nn+f'risidual_lstm_mo')\n",
        "\n",
        "IPython.display.clear_output()\n",
        "val_performance['Residual LSTM'] = residual_lstm.evaluate(wide_window.val)\n",
        "performance['Residual LSTM'] = residual_lstm.evaluate(wide_window.test)\n",
        "print()"
      ],
      "metadata": {
        "id": "qx531nA_UX5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "residual_lstm_v0 = ResidualWrapper(\n",
        "                                tf.keras.Sequential([\n",
        "                                tf.keras.layers.LSTM(32, return_sequences=True),\n",
        "                                tf.keras.layers.Dense(\n",
        "                                    num_features,\n",
        "                                    # The predicted deltas should start small.\n",
        "                                    # Therefore, initialize the output layer with zeros.\n",
        "                                    kernel_initializer=tf.initializers.zeros())\n",
        "                                 ]))"
      ],
      "metadata": {
        "id": "Qsy3Qm-vF_Lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Input shape:', wide_window.example[0].shape)\n",
        "print('Labels shape:', wide_window.example[1].shape)\n",
        "print('Output shape:', residual_lstm_v0(wide_window.example[0]).shape)"
      ],
      "metadata": {
        "id": "6WYJPJE3GzXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "residual_lstm_v0.summary(expand_nested=True)"
      ],
      "metadata": {
        "id": "G7mAnjxWGzTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#history = compile_and_fit(residual_lstm_v0, wide_window)\n",
        "\n",
        "#residual_lstm_v0.save(folders_nn+f'residual_lstm_v0')\n",
        "residual_lstm_v0 = tf.keras.models.load_model(folders_nn+f'residual_lstm_v0')\n",
        "\n",
        "IPython.display.clear_output()\n",
        "val_performance['Residual LSTM_v0'] = residual_lstm_v0.evaluate(wide_window.val)\n",
        "performance['Residual LSTM_v0'] = residual_lstm_v0.evaluate(wide_window.test)"
      ],
      "metadata": {
        "id": "3HCSV2aDGy7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "residual_lstm_v1 = tf.keras.Sequential([tf.keras.layers.LayerNormalization(epsilon=0.001),\n",
        "                                      ResidualWrapper(\n",
        "                                      tf.keras.Sequential([\n",
        "                                      tf.keras.layers.LSTM(32, return_sequences=True),\n",
        "                                      tf.keras.layers.Dense(\n",
        "                                          num_features,\n",
        "                                          # The predicted deltas should start small.\n",
        "                                          # Therefore, initialize the output layer with zeros.\n",
        "                                          kernel_initializer=tf.initializers.zeros())])),\n",
        "                                      tf.keras.layers.LSTM(32, return_sequences=True),\n",
        "                                      tf.keras.layers.Dense(num_features)\n",
        "\n",
        "                                    ])"
      ],
      "metadata": {
        "id": "fN9I7Xy6Il4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Input shape:', wide_window.example[0].shape)\n",
        "print('Labels shape:', wide_window.example[1].shape)\n",
        "print('Output shape:', residual_lstm_v1(wide_window.example[0]).shape)"
      ],
      "metadata": {
        "id": "CC3lTbzQIl1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "residual_lstm_v1.summary(expand_nested=True)"
      ],
      "metadata": {
        "id": "2DbwS6hhI0Rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#history = compile_and_fit(residual_lstm_v1, wide_window)\n",
        "\n",
        "#residual_lstm_v1.save(folders_nn+f'residual_lstm_v1')\n",
        "residual_lstm_v1 = tf.keras.models.load_model(folders_nn+f'residual_lstm_v1')\n",
        "\n",
        "IPython.display.clear_output()\n",
        "val_performance['Residual LSTM_v1'] = residual_lstm_v1.evaluate(wide_window.val)\n",
        "performance['Residual LSTM_v1'] = residual_lstm_v1.evaluate(wide_window.test)\n",
        "print()"
      ],
      "metadata": {
        "id": "m7zTMLbeI0Rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I42Er9Du6co1"
      },
      "source": [
        "#### Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZxR38P_6pUi"
      },
      "source": [
        "Here is the overall performance for these multi-output models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XgTK9tnr7rc"
      },
      "outputs": [],
      "source": [
        "x = np.arange(len(performance))\n",
        "width = 0.3\n",
        "\n",
        "metric_name = 'mean_absolute_error'\n",
        "metric_index = lstm_model.metrics_names.index('mean_absolute_error')\n",
        "val_mae = [v[metric_index] for v in val_performance.values()]\n",
        "test_mae = [v[metric_index] for v in performance.values()]\n",
        "\n",
        "plt.bar(x - 0.17, val_mae, width, label='Validation')\n",
        "plt.bar(x + 0.17, test_mae, width, label='Test')\n",
        "plt.xticks(ticks=x, labels=performance.keys(),\n",
        "           rotation=45)\n",
        "plt.ylabel('MAE (average over all outputs)')\n",
        "plt.grid(linestyle='--',alpha=0.5)\n",
        "_ = plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URz3ajCc6kBj"
      },
      "outputs": [],
      "source": [
        "for name, value in performance.items():\n",
        "  print(f'{name:15s}: {value[1]:0.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Vt2MJhNxwPU"
      },
      "source": [
        "The above performances are averaged across all model outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYokb7Om2YbK"
      },
      "source": [
        "## Multi-step models\n",
        "\n",
        "Both the single-output and multiple-output models in the previous sections made **single time step predictions**, one hour into the future.\n",
        "\n",
        "This section looks at how to expand these models to make **multiple time step predictions**.\n",
        "\n",
        "In a multi-step prediction, the model needs to learn to predict a range of future values. Thus, unlike a single step model, where only a single future point is predicted, a multi-step model predicts a sequence of the future values.\n",
        "\n",
        "There are two rough approaches to this:\n",
        "\n",
        "1. Single shot predictions where the entire time series is predicted at once.\n",
        "2. Autoregressive predictions where the model only makes single step predictions and its output is fed back as its input.\n",
        "\n",
        "In this section all the models will predict **all the features across all output time steps**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFsDAwVt4_rq"
      },
      "source": [
        "For the multi-step model, the training data again consists of hourly samples. However, here, the models will learn to predict 24 hours into the future, given 24 hours of the past.\n",
        "\n",
        "Here is a `Window` object that generates these slices from the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cFYtsz6XiGw"
      },
      "outputs": [],
      "source": [
        "OUT_STEPS = 24\n",
        "multi_window = WindowGenerator(input_width=24,\n",
        "                               label_width=OUT_STEPS,\n",
        "                               shift=OUT_STEPS)\n",
        "\n",
        "multi_window.plot()\n",
        "multi_window"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for example_inputs, example_labels in multi_window.train.take(1):\n",
        "  print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n",
        "  print(f'Labels shape (batch, time, features): {example_labels.shape}')"
      ],
      "metadata": {
        "id": "5tt0GJmNaxpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lg8SInh9Jzd"
      },
      "source": [
        "### Baselines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axwpoWYOApJL"
      },
      "source": [
        "A simple baseline for this task is to repeat the last input time step for the required number of output time steps:\n",
        "\n",
        "[Repeat the last input, for each output step](https://drive.google.com/file/d/1xOUaW_yH7UjBBpm8JT7h6uqOeXweT7TK/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCeQUdi9Z2HD"
      },
      "outputs": [],
      "source": [
        "class MultiStepLastBaseline(tf.keras.Model):\n",
        "  def call(self, inputs):\n",
        "    return tf.tile(inputs[:, -1:, :], [1, OUT_STEPS, 1])\n",
        "\n",
        "last_baseline = MultiStepLastBaseline()\n",
        "last_baseline.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
        "                      metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
        "\n",
        "multi_val_performance = {}\n",
        "multi_performance = {}\n",
        "\n",
        "multi_val_performance['Last'] = last_baseline.evaluate(multi_window.val)\n",
        "multi_performance['Last'] = last_baseline.evaluate(multi_window.test, verbose=0)\n",
        "multi_window.plot(last_baseline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvHZ93ObAfMA"
      },
      "source": [
        "Since this task is to predict 24 hours into the future, given 24 hours of the past, another simple approach is to repeat the previous day, assuming tomorrow will be similar:\n",
        "\n",
        "[Repeat the previous day](images/multistep_repeat.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8Y1uMhGwIRs"
      },
      "outputs": [],
      "source": [
        "class RepeatBaseline(tf.keras.Model):\n",
        "  def call(self, inputs):\n",
        "    return inputs\n",
        "\n",
        "repeat_baseline = RepeatBaseline()\n",
        "repeat_baseline.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
        "                        metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
        "\n",
        "multi_val_performance['Repeat'] = repeat_baseline.evaluate(multi_window.val)\n",
        "multi_performance['Repeat'] = repeat_baseline.evaluate(multi_window.test)\n",
        "multi_window.plot(repeat_baseline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pzD_xzyZ2EB"
      },
      "outputs": [],
      "source": [
        "multi_val_performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrpHR82LZ2AK"
      },
      "outputs": [],
      "source": [
        "multi_performance"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single-shot models\n",
        "\n",
        "One high-level approach to this problem is to use a \"single-shot\" model, where the model makes the entire sequence prediction in a single step.\n",
        "\n",
        "This can be implemented efficiently as a `tf.keras.layers.Dense` with `OUT_STEPS*features` output units. The model just needs to reshape that output to the required `(OUTPUT_STEPS, features)`."
      ],
      "metadata": {
        "id": "qYfjJZrIMQqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Linear\n",
        "\n",
        "A simple linear model based on the last input time step does better than either baseline, but is underpowered. The model needs to predict `OUTPUT_STEPS` time steps, from a single input time step with a linear projection. It can only capture a low-dimensional slice of the behavior, likely based mainly on the time of day and time of year.\n",
        "\n",
        "[Predict all timesteps from the last time-step](https://drive.google.com/file/d/137glUoIA8owbmWh4zm_-5ZcHnm79SEKn/view?usp=sharing)"
      ],
      "metadata": {
        "id": "HFi713YDMe51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "multi_linear_model = tf.keras.Sequential([\n",
        "    # Take the last time-step.\n",
        "    # Shape [batch, time, features] => [batch, 1, features]\n",
        "    tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n",
        "    # Shape => [batch, 1, out_steps*features]\n",
        "    tf.keras.layers.Dense(OUT_STEPS*num_features,\n",
        "                          kernel_initializer=tf.initializers.zeros()),\n",
        "    # Shape => [batch, out_steps, features]\n",
        "    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
        "])"
      ],
      "metadata": {
        "id": "sva81VF6MQc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Input shape:', multi_window.example[0].shape)\n",
        "print('Labels shape:', multi_window.example[1].shape)\n",
        "print('Output shape:', multi_linear_model(multi_window.example[0]).shape)"
      ],
      "metadata": {
        "id": "lILBTMaHNtA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multi_linear_model.summary()"
      ],
      "metadata": {
        "id": "2qS-zo5dMQW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = compile_and_fit(multi_linear_model, multi_window)\n",
        "\n",
        "IPython.display.clear_output()\n",
        "multi_val_performance['Linear'] = multi_linear_model.evaluate(multi_window.val)\n",
        "multi_performance['Linear'] = multi_linear_model.evaluate(multi_window.test, verbose=0)\n",
        "multi_window.plot(multi_linear_model)"
      ],
      "metadata": {
        "id": "3kqvlruGMQTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YNq4ojrCMQQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xNAX1_WTMQNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qusohj2CMQKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SF4vcNXeMQGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0puSlSHlMQCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CHFcSbwZ18P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irJ110CBuktq"
      },
      "outputs": [],
      "source": [
        "history = compile_and_fit(lstm_model, wide_window)\n",
        "\n",
        "IPython.display.clear_output()\n",
        "val_performance['LSTM'] = lstm_model.evaluate( wide_window.val)\n",
        "performance['LSTM'] = lstm_model.evaluate( wide_window.test, verbose=0)\n",
        "\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0lHDLNjukqj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4nWHpTRukfD"
      },
      "outputs": [],
      "source": [
        "a,b = wide_window.example\n",
        "a.shape, b.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1Lf7OVeu0PN"
      },
      "outputs": [],
      "source": [
        "a_ = tf.keras.layers.AveragePooling1D(pool_size=3, strides=1, padding=\"same\",data_format=\"channels_last\")(a)\n",
        "#a_ = tf.keras.layers.UpSampling1D(size=2)(a)\n",
        "#a_ = tf.keras.layers.Cropping1D(cropping=(2, 10))(a)\n",
        "a_.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLdsDoR_uuRT"
      },
      "outputs": [],
      "source": [
        "plt.plot(a[0,:,2], c=\"r\")\n",
        "plt.plot(a_[0,:,2], c=\"royalblue\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvvpHxlye7sn"
      },
      "outputs": [],
      "source": [
        "val_performance['Dense'] = dense.evaluate(single_step_window.val)\n",
        "performance['Dense'] = dense.evaluate(single_step_window.test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxmtL3VdvOxD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNqXURWPuGJGnr8PwxLTpEe",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}